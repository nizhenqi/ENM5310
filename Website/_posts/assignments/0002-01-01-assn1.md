---
layout: assignment
categories:
 - assignment
title: "Assignment #1: Probability and Statistics (due 02/03)"
due:
tags:
 - week-2
permalink: /assn1/
---

## Problem 1 [20 Points]

### Subproblem 1 [10 Points]

Consider a coin that comes up heads with probability $$p$$ and tails with probability $$1-p$$. Let $$q_n$$ be the probability of obtaining even number of heads in $$n$$ independent tosses. Derive a recursion that relates $$q_n$$ to $$q_{n-1}$$ and establish the formula

$$
q_n = \frac{1+(1-2p)^n}{2}.
$$


### Subproblem 2 [10 Points]

Let $$X$$ and $$Y$$ have joint PDF 
$$
f_{X,Y}(x,y) = \begin{cases}
C e^{-(ax+by)} & x,y \geq 0\\
0 & \text{otherwise}
\end{cases},
$$
where, $$a, b > 0$$ are constants.

- Determine the constant $$C$$.
- Find the marginal density of $$X$$ and $$Y$$. What can you infer from them?
- Find $$\mathbb{E}(Y \mid X> \frac{\exp(a^2 + b^2)}{a^4 + b^4})$$.

## Problem 2 [20 points]

Let $$S_1,S_2,\ldots,S_n$$ be a partition of the sample space $$\Omega$$.
- Show that for any event $$A$$,
  $$\mathbb{P}(A) = \sum_{i=1}^n \mathbb{P}\left({A \cap S_i}\right)$$.
- Use the previous part to show that, for events $$A$$, $$B$$ and $$C$$,
  $$\mathbb{P}\left({A}\right) = \mathbb{P}\left({A \cap B}\right) + \mathbb{P}\left({A \cap C}\right) + \mathbb{P}\left({A \cap B^c \cap C^c}\right) - \mathbb{P}\left({A \cap B \cap C}\right)$$.
- Prove that for any two events $$A$$ and $$B$$, we have
$$\mathbb{P} (A \cap B) \geq \mathbb{P}(A) + \mathbb{P}(B) - 1$$.
- Using the above, establish the following generalization:
$$\mathbb{P}(A_1 \cap A_2 \cap \cdots \cap A_n) \geq \mathbb{P}(A_1) + \mathbb{P}(A_2) + \cdots + \mathbb{P}(A_n) - (n-1)$$.

**[You will need to argue logically using only the basic axioms of probability. Drawing a diagram is not enough.]**



## Problem 3 [20 Points]

### Subproblem 1 [10 Points]

- If $$X_1,X_2,\ldots,X_n$$ are independent random variables having the same probability density function $$f_X(x)$$, what is the probability density function for the random variable $$Y=\text{min}\{X_1,X_2,\ldots,X_n\}$$?
- Consider two continuous random variables $$Y$$ and $$Z$$ and a random variable $$X$$ that is equal to $$Y$$ with a probability $$p$$ and equals $$Z$$ with a probability $$1-p$$. Obtain the pdf of $$X$$ interms of the pdf's of $$Y$$ and $$Z$$.

### Subproblem 2 [10 Points]

The Laplace distribution is given by
$$p(x | \mu, b) = \frac{1}{2 b} \exp \left(-\frac{|x - \mu|}{b}\right)$$.

Consider a mixture of three Laplace distributions:
$$p(x) = \alpha p_1(x) + \beta p_2(x) + \gamma p_3(x)$$,
where $$\alpha, \beta, \gamma \in [0,1]$$ are mixture weights satisfying $$\alpha + \beta + \gamma = 1$$ and $$p_1(x)$$, $$p_2(x)$$ and $$p_3(x)$$ are Laplace distributions with different parameters $$(\mu_1, b_1) \neq (\mu_2, b_2) \neq (\mu_3, b_3)$$.

Derive the expectation and variance of $$p(x)$$, analytically, using their definitions.


## Problem 4 [20 Points]

As mentioned in class the Gaussian has nice properties which makes it a fundamental tool in statistical infererence. The standard normal $$\mathcal{N}(\mathbf{x}; \mathbf{\mu}, \mathbf{\Sigma})$$ is defined as

$$
    \mathcal{N}(\mathbf{x}; \mathbf{\mu}, \mathbf{\Sigma}) = \frac{1}{\sqrt{(2 \pi)^n |\mathbf{\Sigma}|}} \exp \left[ -\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^\top \Sigma^{-1} (\mathbf{x} - \mathbf{\mu}) \right].
$$

- Prove that if $$x \in \mathbb{R}^d$$ is normally distributed, every affine transformation $$y = A x + b$$ also has a Gaussian distribution. Find its mean and covariance.
- Analytically find the KL divergence $$\mathbb{KL}(P \lvert\lvert Q)$$ between two multivariate normal distributions $$p(x) \sim \mathcal{N}(\mathbf{x}; \mathbf{\mu}_1, \mathbf{\Sigma}_1)$$ and $$q(x) \sim \mathcal{N}(\mathbf{x}; \mathbf{\mu}_2, \mathbf{\Sigma}_2)$$.


## Problem 5 [20 Points]

Consider a two-dimensional random variable $$z=(z_1,z_2)$$ distributed as

$$
p(z_1,z_2) = \mathcal{N}\left(\left[\begin{matrix} 0 \\ 0 \end{matrix}\right],
\left[\begin{matrix} 1 & 0 \\ 0 & 1 \end{matrix}\right]\right).
$$


Also consider a transformation $$z_1 = g_1(x_1) = x_1^3 - 6x_1^2 + 12x_1 - 8$$ whose inverse mapping is $$x_1 = g_1^{-1}(z_1) = z_1^{1/3} + 2$$, and $$z_2 = g_2(x_2) = x_2 + 1$$ whose inverse mapping is $$x_2 = g_2^{-1}(z_2) = z_2 - 1$$. Generate $$5000$$ realizations for $$(z_1, z_2)$$ from $$p(z_1,z_2)$$ and compute the corresponding $$(x_1, x_2)$$ pairs according to the given transformation. Then, maximize the log-likelihood of the $$(x_1, x_2)$$ observations to estimate the parameters of the forward mapping that transforms $$p(z)$$ into $$p(x)$$ (consider a polynomial basis). Report the learned parameters, log-likelihood loss and a visualization of $$p(x)$$ at every step of your gradient ascent algorithm.